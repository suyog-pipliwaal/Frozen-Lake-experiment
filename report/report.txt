Assignmemt 1

Suyog Pipliwal 210634338
Nirmal
Razhan Hameed 210667222
November 4, 2021

Abstract

Poommerman is a boardgame drived from a famous Ninetendo game called Bomberman. The goal of game
is destory the other players by bombs but the path is blocked by walls, wooden or rigid wall. Wood walls
can be destroy using bomb but not the rigid walls. Each player start at one corner of the board and walls
are generated randomly. After destorying a walls you might get a power up which further make the game
interseting. The objective of this paper is to desgin or modify an existing agent and analysis the this agent
and achive high winning percentage agaist other players.

Contents

1 Introduction
1.1

1.2

3

Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3

1.1.1

Types of Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4

Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5

1.2.1

5

Types of Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Background
2.1

Pommerman
2.1.1

7
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

Types of Player . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8

3 Method

9

3.1

Opponent Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9

3.2

RHEA Parameter Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10

3.2.1

Genetic Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10

3.2.2

Fitness Assignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10

3.2.3

Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10

3.2.4

Shift Buffer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11

4 Experimental Study
4.1

12

Experiment Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

12

5 Discussion

14

6 Conclusions and Future Work

15

2

Chapter 1

Introduction
Games are ideal environments for agents as they provide realistic environment in which only limited information is available and where decisions must be made under time and pressure constraints. There are
many types of methods that are used to make decision make sure that agents is behave like a human begin. Examples like Monte Carlo Tree search[1], Rolling Horizon Evolutionary Algorithm[2], Finite State
Machine(FSM)[3], MiniMax[4], Artificial Neural Networks[5] etc. The motive of all methods is build a agent
that can perform well under any circumstance. This report is part of assignment for AI in game module.
The objective of this assignment is to create or modify agent and compete with other agent is framwork.
Before that we need to understand what is agent mean and how to construct one.

1.1

Agent

Agent is anything that can understand it’s surrounding environment through it sensors, can reson about it’s
goal and act for the same in the given evnironment. Building an agent require knowledge of foue things as
mention below:• Performance Measure:- To evaluates our agent is doing what it meant to do.
• Environments:- The domain in which our agent is meant to working.
3

• Actuators:- Means by which agent acts upon environment.
• Sensors:- Input device by which our agent get to know about environment.
In simple terms a agent is map between percept and action.
f : P ∗− > A

where P* is complete history of it’s percept and A is the action taken by agent.

1.1.1

Types of Agent

Agents are classified based on the degree of there percept and capability. They are define as below:• Simple reflex agent: Agent that take decision based on current percept. The agent function is based
on the condition-action rule. A condition-action rule is a rule that maps a state i.e, condition to an
action. If the condition is true, then the action is taken, else not. This agent function only succeeds
when the environment is fully observable.
• Model-Based Reflex Agents: A model-based reflex agent is one that uses its percept history and its
internal memory to make decisions about an internal ”model” of the world around it. Internal memory
allows these agents to store some of their navigation history, and then use that semi-subjective history
to help understand things about their current environment
• Goal-Based Agents: A model-based reflex agent is one that uses its percept history and its internal
memory to make decisions about an internal ”model” of the world around it. Internal memory allows
these agents to store some of their navigation history, and then use that semi-subjective history to
help understand things about their current environment. Their every action is intended to reduce its
distance from the goal. This allows the agent a way to choose among multiple possibilities, selecting
the one which reaches a goal state. The knowledge that supports its decisions is represented explicitly
and can be modified, which makes these agents more flexible.

4

• Utility-Based Agents: The agents which are developed having their end uses as building blocks are
called utility-based agents. When there are multiple possible alternatives, then to decide which one
is best, utility-based agents are used. They choose actions based on a preference (utility) for each
state.Utility describes how “happy” the agent is. Because of the uncertainty in the world, a utility
agent chooses the action that maximizes the expected utility.
• Learning Agent: A learning agent in AI is the type of agent that can learn from its past experiences
or it has learning capabilities. It starts to act with basic knowledge and then is able to act and adapt
automatically through learning.

1.2

Environment

Understanding of the environement is equally important also, a agent take input from it’s surrounding
and output a suitable action. Environment are classified into many category based on the property they
Possessed.

1.2.1

Types of Environment

• Fully Observable vs Partially Observable If an agent’s sensors give it access to the complete state
of the environment at each point in time, then we say that the task environ- ment is fully observable.
A task environment is effectively fully observable if the sensors detect all aspects that are relevant to
the choice of action; relevance, in turn, depends on the performance measure.
• Deterministic vs Stochastic If the next state of the environment is completely determined by the
current state and the action executed by the agent, then we say the environment is deterministic
otherwise, it is stochastic.
• Competitive vs Collaborative An agent is said to be in a competitive environment when it competes
against another agent to optimize the output. For e.g Poommerman in free mode is a competitive

5

environment. But if all the agent in an environment are collaborating to produce a result it is called
Collaborative environment.
• Single-agent vs Multi-agent When there is single agent operating in the given environment it is
called single-agent environment. If there are multiple agent it called multi-agent environment. For e.g
a chess game is a multi-agent whereas a taxi driver is single agent environment if we consider other
vechiles as object.
• Static vs Dynamic An environment that keeps constantly changing itself when the agent is up with
some action is said to be dynamic. An idle environment with no change in its state is called a static
environment.
• Discrete vs Continuous If an environment consists of a finite number of actions that can be deliberated in the environment to obtain the output, it is said to be a discrete environment.The environment
in which the actions performed cannot be numbered ie. is not discrete, is said to be continuous.

In pommerman we have four player DoNothing, Random,One-steo look ahead(OSLA), SimplePlayer, RHEA,MCTS DoNothing, Random and Simple agent come under Simple reflex agent. MCTS
and RHEA are under goal based agent. The environment is discreate, single-agent, Competitive as
well as fully and partially observable. The observable of environment can be change with parameter as
per requirement. We will learn more about them in next chapter.

6

Chapter 2

Background
We have now a basic understanding of a agent and environment and now we need a playground to play
and test our strategy. We chose Pommerman as our playground because it provide a rich and challenging
environment for a agent.

2.1

Pommerman

Pommerman is a board game on based on Bomberman. Board is of dimension 11x11 and there are four
agent in each corner. The board is filled with wood and rigid walls. Rigid walls are impassable and cant
be destroy where as wooden walls can be destroy with bombs. Each player 2 bombs and with help of this
player destroy wooden walls and destroying walls get power-up like extra bomb etc.
There are two modes and four players in the game which with its own rules and flavors. Each mode
has it’s own challange and contraints which provide perfect playground to test your agent. The objective of
game is to find a way to your opponent by destroying wooden walls and then destroy your opponent with
bomb and also checking for power-up in the path.
• FFA Mode: Free For All mode let all the player agaist each other and one of them is gonna be winner
of game. Every agents have have there own planning and tactis for this mode and the board is fully

7

observerable for all agents.
• Team Mode: It a 2v2 team mode where one team wins and teams usually have partial observable.
Player in opposite corner of the board form a party and try to beat the other party. If both player of
a party die that paty loose and other win.

2.1.1

Types of Player

There are six types of player in pommerman as explain below:1. MCTS Player:- Items are numbered automatically.
2. RHEA Player:- The numbers start at 1 with each use of the enumerate environment.
3. Simple Player:- Another entry in the list
4. One Step Look Ahead(OSLA):-

8

Chapter 3

Method
Our method consists of a combination of Opponent Modeling and fine tuning of the major parameters of
the RHEA algorithm

3.1

Opponent Modeling

Opponent modeling is determing the what decision the opponent will make give each state. Opponent
modeling is useful in this particular case of Pommerman, if we reduce the vision range; meaning the agent
does not have perfect information about the game board. There different stratigies to be depolyed to model
the opponent’s action, in our method we chose to learn the model. We collected data to adapt it to different
players. In our case we ran the for 4 hours, we collected 2 hours of data for MCTS and 2 more hours for
OLSA.
Below are the steps we took to build our Opponent Modeling:
1. In the first step we collected data by running the game for 2 hours making all the agnets the same;
say MCTS for example. Then we stored every agent action in each state into a csv file.
2. Then we implemented Bayes rule, and applied it to the prior distribution based on the collected data
and adapting it to specific opponent.
9

3. Finally we integrated the model to the game through the Game Interface.

We will discuss the results of our opponent modeling in the next chapter.

3.2

RHEA Parameter Space

In this section we will explain the parameters we explored and tuned for the algorithm to perform better.

3.2.1

Genetic Operators

There are three main genetic operators used by the evolutionary algorithm in RHEA: crossover, selection and
mutation[6]. In selection we chose Tournament Selection with tournament size being 50 percent chosen
from the population. Among the two types of crossovers that are available: uniform, and nbit, we chose
uniform. Uniform selects actions from each parent with equal probability[6]. As for the mutation we chose
Uniform Mutation with mutation rate of 0.3 and mutation gene count of 2. Uniform mutation assigns
each gene an equal probability of mutation (m = 1/L, where L is the individual length) and picks a different
value for the genes mutating uniformly at random [6].

3.2.2

Fitness Assignment

For the evaluation we chose to go with keeping a discounted sum of all values when translating a game
state into a fitness value. In out implementtation the model have evaluation discount = 0.95 which values
immediate reward. Evaluating individuals plays an important role in chosing the next generation.

3.2.3

Initialization

We initialize our algorithm with MCTS. We set the MCTS budget to 50 percent, and 12 node as the MCTS
depth. This technique uses a tree to chose an action for the first level of play. MCTS iteratively builds a
search tree by selecting a node with the highest Q(s, a). 1

10

3.2.4

Shift Buffer

”This is a population management technique which avoids repeating the entire search process from scratch
at every new game tick, which usually loses information gained in previous iterations of the algorithm. It
works by keeping the final population evolved during one game tick to the next. However, as the first action
of the best individual has just been played, all first actions from all individuals in the population are removed
and a new random action is added at the end.” [6].

11

Chapter 4

Experimental Study
4.1

Experiment Setup

In our setup we tried to replicate a smaller set of experimentaions provided by this paper[7].
In the setup each player plays for 10 level and 5 repetitions for each level, which makes it 50 games per
configuration. We play in two different game modes, Free For All and TEAM in three different observability
settings; 2, 4, and full observability.
table 1
Free For All
VR j

Agents

all

RHEA vs OSLA vs OSLA vs OSLA
RHEA vs RuleBased vs RuleBased vs RuleBased
RHEA vs MCTS vs MCTS vs MCTS
(vanilla) RHEA vs OLSA vs OLSA vs OLSA

12

Team Mode
VR j

Agents

all

RHEA x 2 vs OSLA x 2
RHEA x 2 vs RuleBased x 2
RHEA x 2 vs MCTS x 2
RHEA x 2 vs MCTS and OSLA

RHEA uses 200 budget iterations with opponent modeling using Naive Bayes. We left the default
implementation of the custom heuristic that computes the score relative to the root’s state. In the FFA mode
we ran the vanilla version of RHEA with a uniform random opponent model against OLSA to determine
how much we have improved our agent over the base implementation.

13

Chapter 5

Discussion

14

Chapter 6

Conclusions and Future Work

15

Bibliography
[1] C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener, D. Perez,
S. Samothrakis, and S. Colton, “A survey of monte carlo tree search methods,” IEEE Transactions on
Computational Intelligence and AI in games, vol. 4, no. 1, pp. 1–43, 2012.
[2] R. D. Gaina, “Rolling horizon evolutionary algorithm improvements for general video game programming
in single and multi-player games,”
[3] A. Gill et al., “Introduction to the theory of finite-state machines,” 1962.
[4] A. M. Turing, “Digital computers applied to games. faster than thought,” 1953.
[5] K. Hornik, M. Stinchcombe, and H. White, “Multilayer feedforward networks are universal approximators,” Neural networks, vol. 2, no. 5, pp. 359–366, 1989.
[6] R. D. Gaina, S. Devlin, S. M. Lucas, and D. Perez, “Rolling horizon evolutionary algorithms for general
video game playing,” IEEE Transactions on Games, 2021.
[7] D. Perez-Liebana, R. D. Gaina, O. Drageset, E. Ilhan, M. Balla, and S. M. Lucas, “Analysis of statistical forward planning methods in pommerman,” in Proceedings of the AAAI Conference on Artificial
Intelligence and Interactive Digital Entertainment, vol. 15, pp. 66–72, 2019.

16

