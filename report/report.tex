\documentclass{report}

\usepackage{fullpage}
\usepackage{tabularx}

\renewcommand{\baselinestretch}{2}

\author{
	Suyog Pipliwal 210634338 \\
	Nirmalkumar Pajany 210239797 \\
	Razhan Hussein Hameed 210667222
}



\title{Assignment 1 Pommerman}


\begin{document}
    %TC:ignore
	\maketitle
	\begin{abstract}
		Pommerman is a board-game derived from a famous Nintendo game called Bomber-man. The goal of game is destroy the other players by bombs. This report is for assignment in AI in games module. The aim of this report is to modify an RHEA algorithm with opponent modelling in order to improve winning percentage. We achieve this by fine tuning RHEA parameters and developing our own opponent modelling strategy. 
		
	\end{abstract}
	
	\tableofcontents
	
	%TC:endignore
	\chapter{Introduction}
	Games are ideal environments for agents as they provide a realistic environment in which only limited information is available and where decisions must be made under time and pressure constraints. 
	The framework we are working is called Pommerman. It is perfect to test our methods because of it's complexity and rich environment it provide. There are many existing methods that are available decision in Pommerman like Monte Carlo Tree search\cite{browne2012survey}, Rolling Horizon Evolutionary Algorithm\cite{gaina2021rolling} etc. The rest of the report will explain Pommerman, methods, and the results we obtained . 
	\section{Pommerman}
	Pommerman is a board game based on Bomber-man. Board is of dimension 11x11 and there are four agent in each corner. The board is filled with wood and rigid walls. Rigid walls are impassable and cant be destroy where as wooden walls can be destroy with bombs. Each player has 2 bombs, the player can destroy wooden walls. when players destroy walls they get power-ups like extra bombs etc. 
	
	There are two modes and four players in the game. Each mode has it is own challenges and constraints which provide the perfect playground to test your agent.  The objective of the game is to find a way to your opponent by destroying wooden walls and then destroying your opponent with bombs and also checking for power-up in the path.  
	\begin{itemize}
		\item FFA Mode: Free For All mode let all the players play against each other and one of them is gonna be the winner of the game. Every agent has their own planning and tactics for this mode and the board may or may not be observable for all agents. 
		\item Team Mode: It is a 2v2 team mode where one team wins and teams may or may not have partial observability of the board. The players in the opposite corner of the board form a team and try to beat the other team. If both players of a team die, that team loses the game and the other team win.
	\end{itemize}
	\section{Types of Agents}
	There are six types of agents implemented in Pommerman as explained below:- 
	\begin{enumerate}
	\item \textbf{MCTS Player}:- This player uses the Monto Carlo tree search(MCTS) algorithm to find the best action for the player. MCTS is a tree search algorithm where it finds optimal decisions in the given environment using roll-out and state evaluation. In the decision space and building a search tree according to the results\cite{browne2012survey}. In Pommerman, MCTS creates a root node and then implements tree policy and finally roll-out. After final roll-out we get an action that is best suitable for the given game state.
	\item \textbf{RHEA Player}:- This player uses Rolling Horizon Evolutionary Algorithms\cite{gaina2021rolling} (RHEA) for finding the best decision in the given game state. RHEA is a family of evolutionary computation methods for decision making in real-time. The algorithm starts with a population seeding. The population consist of individual and each individual contains genes(actions). Each individual generates offspring based on a genetic operator passed as a parameter to the algorithm. Each individual is evaluated and the best one is chosen\cite{slide}. 
	\item \textbf{Rule Based Player}:- {Rule Based Player}:- This player takes action based on a set of certain rules.
	\item \textbf{One Step Look Ahead(OSLA)}:- This player calculate the fitness value of each possible action and  return the action corresponding to maximum fitness value.
	\item\textbf{Random}:- This player picks an action randomly. 
	\item\textbf{DoNothing}:- This player does nothing as name suggest.
\end{enumerate}
	\chapter{Background}
	In this chapter, we will explain the Rolling Horizon Evolutionary Algorithms(RHEA), and the reason we chose this algorithm for our study. RHEA belong to a class of evaluation algorithm where the algorithm looks for global maxi-ma or mini-ma in the given state. Unlike many other searches that look at some point in state RHEA look for multiple points in order to get the best solution.
	\section{RHEA}
	Rolling Horizon Evolutionary Algorithm (RHEA) is a family of evolutionary computation methods developed for real-time decision making. Similar to Monte Carlo Tree Search (MCTS), it is an anytime algorithm, which means it can be stopped after running several iterations and it will still return a sensible action for the game. 
	Steps followed in RHEA are as follow:-
	\subsection{Procedure}
	The algorithms start with an initial population of n individuals. Each individual contains a set of actions that can lead to a goal state. Now we have to select an individual for performing actions that can win us the game.\cite{gaina2017analysis}
	\begin{enumerate}
	    \item \textbf{Selection}: Select some individuals randomly from the population and evaluate their fitness. An individual with the highest fitness value is selected.
	    \item \textbf{Crossover}:- The selected individuals are then crossed over to generate offspring. There are many types crossover like uniform crossover, 1-point crossover, 2-point crossover. 
	    \item \textbf{Mutation}:- Mutation can be defined as a small, random change to the solution. Mutation is applied so that the individual can be differentiated from the rest of the individuals in the sample space.
	    \item \textbf{Individual Roll-out}:- The above steps terminate when an end condition is satisfied, such as a time or memory limit reached or a certain number of iterations have been performed. The action selected by the algorithm is represented by the first gene in the best individual found at the end of the iteration.
	\end{enumerate}
		
	
	\chapter{Method}

	Our method involves a combination of Opponent Modeling and fine-tuning of the major parameters of the RHEA algorithm

	\section{Opponent Modeling}
	Opponent modeling is predicting what decision the opponent will make given each state.
	Opponent modeling is useful in the Pommerman case. When the vision of our agent is reduced, it will not have perfect information about the game board. We supplemented this with opponent modeling. We tried multiple ways to model the opponent, unfortunately, due to the complexity of the implementation we could not have a fully functioning code for all the implementations; nonetheless, we included all the code we wrote with a fully working opponent model.



	There are three different strategies that could be deployed to model the opponent's action: learning a model, hard coding game knowledge in heuristic function, and randomly choosing an action for the opponent. Below are the methods we explored and tried to implement:

	\subsection {Opponent Modeling with Naive Bayes}
	First, we tried to learn the model. We collected data to adapt it to different players. We ran the game for 6 hours, collected 2 hours of data for MCTS, 2 hours of data for vanilla RHEA and 2 more hours for OSLA. Due to the complexity of the method, we could not integrate our implementation with the framework, yet we still included the collected data and our implementation of the model.

	Below are the steps we took to build our Opponent Model.
	
	\begin{enumerate}
	 \item In the first step we collected data by running the game for 6 hours making all the agents the same; say MCTS for example. Then we stored every agent action in each state into a CSV file.
	 \item Then we implemented the Bayes rule and applied it to the prior distribution based on the collected data and adapted it to a specific opponent.
	 \item Finally, we integrated the model into the game through the Game Interface.
	\end{enumerate}


	\subsection{Knowledge about the game}
	We implemented a fully functional mechanism to determine if our enemy is adjacent to us and if they are we assume that they are laying a bomb. Given this, our agent could dodge the bomb every time. Again, the complexity of the implementation made our agent slow. Yet, we decided to submit the code but not include it in the final version of our agent.


	\subsection {Randomness with weights}
	The final approach towards opponent modeling is weighted randomness. We implemented a function to dynamically give specific weight to one or more actions that appear more frequently than others. This approach is in the final version of our agent. Through trial and error, we realized that if we give a higher weight to bomb action, the performance of our agent increases. So, we set the weight of the bomb action to \%35 and the rest of the actions had a weight of \%15 each.


    \section{RHEA Parameter Space}
    In this section, we will explain the parameters we explored and tuned for the algorithm to perform better.

    \subsection{Genetic Operators}
    There are three main genetic operators used by the evolutionary algorithm in RHEA: crossover, selection, and mutation\cite{gaina2021rolling}.
    In selection, we chose \textbf{Selection Rank} which is based on the probability for each individual corresponding to its rank value. Among the two types of crossovers that are available: uniform and nbit, we chose uniform. Uniform selects actions from each parent with equal probability\cite{gaina2021rolling}. For mutation, we chose \textbf{Uniform Mutation} with the mutation rate of 0.3 and mutation gene count of 2.
    Uniform mutation assigns each gene an equal probability of mutation (m = 1/L, where L is the individual length) and picks a different value for the genes mutating uniformly at random \cite{gaina2021rolling}.




	 \subsection {Fitness Assignment}
    For evaluation, we chose to go with keeping a discounted sum of all values when translating a game state into a fitness value. In our implementation, the model has an evaluation discount of 0.95 which values immediate reward. Evaluating individuals play an important role in choosing the next generation.


	 \subsection {Initialization}
    We initialize our algorithm with MCTS. We set the MCTS budget to 50 percent, and 12 nodes as the MCTS depth. This technique uses a tree to choose an action for the first level of play. MCTS iterates to build a search tree by selecting a node with the highest $Q(s,a)$.

	 \subsection  {Shift Buffer}
	 "This is a population management technique which avoids
    repeating the entire search process from scratch at every
    new game tick, which usually loses information gained in
    previous iterations of the algorithm. It works by keeping the final population evolved during
    one game tick to the next. However, as the first action of
    the best individual has just been played, all first actions from
    all individuals in the population are removed and a new
    random action is added at the end." \cite{gaina2021rolling}.
	\chapter{Experimental Study}
	\section{Experiment Setup}
	In our setup, we tried to replicate a smaller set of experimentation's provided by this paper\cite{perez2019analysis}.

	In the setup, each player plays for 10 levels and 5 repetitions for each level, which makes it 50 games per configuration. We play in two different game modes, Free For All and TEAM in three different observably settings; 2, 4, and full observably.

Table 1 for Experiment study set All $\equiv VR \in \{2, 4, \infty\}$. \textbf{Game Mode: FFA}
\newline
%TC:ignore
\begin{center}
	\begin{tabularx}{300pt}{|c|X|c|X|}
	\hline
	VR & Agent \\
	\hline
	all  & RHEA vs OSLA vs OSLA vs OSLA \\
	\hline
	all & RHEA vs RuleBased vs RuleBased vs RuleBased\\
	\hline
	all & RHEA vs MCTS vs MCTS vs MCTS\\
	\hline
	all & (vanilla) RHEA vs OLSA vs OLSA vs OLS\\
	\hline
\end{tabularx}
\newline
\newline
\newline
\newline
\newline
\newline
\textbf{Game Mode: TEAM Mode}
\newline
\newline
\begin{tabularx}{300pt}{|c|X|c|X|}
	\hline
    VR & Agents  \\
    \hline
    all & RHEA x 2 vs OSLA x 2  \\
    \hline
     all & RHEA x 2 vs RuleBased x 2 \\ 
     \hline
     all & RHEA x 2 vs MCTS x 2\\ 
     \hline
     all & RHEA x 2 vs MCTS and OSLA \\
    \hline
\end{tabularx}
\end{center}
%TC:endignore


    RHEA uses budget iterations of 200 with opponent modeling using \textbf{ Weighted Randomness}. We left the default implementation of the custom heuristic that computes the score relative to the root's state.
    In the FFA mode, we ran the vanilla version of RHEA with a uniform random opponent model against OSLA  to determine how much we have improved our agent over the base implementation.



     Table 2 for Experiment study set All  vision range in $ \in \{2, 4, \infty\}$. Game is begin played in \textbf{team mode}
     \newline
     %TC:ignore
     \begin{center}
     \textbf{Opponent: OSLA}
     \begin{tabularx}{330pt}{|c|X|c|c|c|}
     	\hline
     	VR & Agents & \%Win & \%Ties & \%Loss\\
     	\hline
     	2 & RHEA RHEAS & 40\% &10\% &50\%\\
     	\hline
     	4 & RHEA RHEA  & 48\% &4\% &48\% \\
     	\hline
     	 $\infty$ &RHEA RHEA & 50\%&4\% &46\% \\
     	\hline
     \end{tabularx}\newline

 \textbf{Opponent: RULE}
 	 \begin{tabularx}{330pt}{|c|X|c|c|c|}
 	\hline
 	VR & Agents & \%Win & \%Ties & \%Loss\\
 	\hline
 	2 & RHEA RHEA & 48.0\%	&10.0\%	&42.0\%\\
 	\hline
 	4 & RHEA RHEA & 52.0\%	&6.0\%	&42.0\%\\
 	\hline
 	$\infty$ &RHEA RHEA &46.0\%	&10.0\%	&44.0\% \\
 	\hline
 \end{tabularx}
\newline
\newline
\textbf{Opponent: MCTS}
\begin{tabularx}{330pt}{|c|X|c|c|c|}
	\hline
	VR & Agents & \%Win & \%Ties & \%Loss \\
	\hline
	2 & RHEA  RHEA &30.0\%	&34.0\%	&36.0\%\\
	\hline
	4 & RHEA  RHEA& 36.0\%	&28.0\% &	36.0\%\\
	\hline
	$\infty$ &RHEA  RHEA & 34.0\%	&20.0\%	&46.0\%\\
	\hline
\end{tabularx}
\newline
\newline
\newline
\newline
\newline
\textbf{Opponent: MCTS+OSLA}
\begin{tabularx}{330pt}{|c|X|c|c|c|}
	\hline
	VR & Agents & \%Win & \%Ties & \%Loss \\
	\hline
	2 & RHEA  RHEA &64.0\%	&10.0\%	&26.0\% \\
	\hline
	4 & RHEA  RHEA& 60.0\%	&10.0\%	&30.0\% \\
	\hline
	$\infty$ &RHEA  RHEA & 60.0\%	&6.0\%	&34.0\%\\
	\hline
\end{tabularx}\newline\newline
%TC:endignore
Table 3 for Experiment study set All  vision range in $ \in \{2, 4, \infty\}$. Game is begin played in FFA mode.\newline
%TC:ignore
\begin{tabularx}{400pt}{|c|X|c|c|c|}
	\hline
	VR& Agents & \%Win & \%Ties & \%Loss\\
	\hline
	2 & RHEA vs OSLA vs OSLA vs OSLA & 88.0&0.0  &12.0\\
	\hline
	4&  RHEA vs OSLA vs OSLA vs OSLA& 94.0&0.0 &6.0 \\
	\hline
	$\infty$  & RHEA vs OSLA vs OSLA vs OSLA & 96.0&0.0 &4.0 \\
	\hline
\end{tabularx}
\begin{tabularx}{400pt}{|c|X|c|c|c|}
	\hline
	VR& Agents & \%Win & \%Ties & \%Loss\\
	\hline
	2 & RHEA vs RuleBased vs RuleBased vs RuleBased & 68.0&2.0 &30.0 \\
	\hline
	4&RHEA vs RuleBased vs RuleBased vs RuleBased  & 84.0&4.0 &12.0 \\
	\hline
	$\infty$ &RHEA vs RuleBased vs RuleBased vs RuleBased  & 82.0&6.0 &12.0 \\
	\hline
\end{tabularx} 
\begin{tabularx}{400pt}{|c|X|c|c|c|}
	\hline
	VR& Agents & \%Win & \%Ties & \%Loss\\
	\hline
	2 & RHEA vs MCTS vs MCTS vs MCTS & 22.0&6.0 &72.0 \\
	\hline
	4&RHEA vs MCTS vs MCTS vs MCTS & 28.0&35.0 &38.0 \\
	\hline
	$\infty$ &RHEA vs MCTS vs MCTS vs MCTS & 24.0&22.0 &54.0 \\
	\hline
\end{tabularx} 
\begin{tabularx}{400pt}{|c|X|c|c|c|}
	\hline
	VR& Agents & \%Win & \%Ties & \%Loss\\
	\hline
	2 & (vanilla)RHEA vs OSLA vs OSLA vs OSLA & 62.0&2.0 &36.0 \\
	\hline
	4& (vanilla)RHEA vs OSLA vs OSLA vs OSLA  & 68.0&10.0 &22.0 \\
	\hline
	$\infty$ & (vanilla)RHEA vs OSLA vs OSLA vs OSLA & 48.0&2.0 &50.0 \\
	\hline
\end{tabularx} 
\begin{tabularx}{400pt}{|c|X|c|c|c|}
	\hline
	VR& Agents & \%Win & \%Ties & \%Loss\\
	\hline
	2 & (vanilla)RHEA vs Rule Based vs Rule Based vs Rule Based & 36.0&6.0 &46.0 \\
	\hline
	4& (vanilla)RHEA vs Rule Based vs Rule Based vs Rule Based  & 40.0&4.0 &56.0 \\
	\hline
	$\infty$ & (vanilla)RHEA vs Rule Based vs Rule Based vs Rule Based & 50.0&4.0 &46.0 \\
	\hline
\end{tabularx} 
\begin{tabularx}{400pt}{|c|X|c|c|c|}
	\hline
	VR& Agents & \%Win & \%Ties & \%Loss\\
	\hline
	2 & (vanilla)RHEA vs MCTS vs MCTS vs MCTS & 4.0&0.0 &96.0 \\
	\hline
	4& (vanilla)RHEA vs MCTS vs MCTS vs MCTS  & 12.0&4.0 &80.0 \\
	\hline
	$\infty$ & (vanilla)RHEA vs MCTS vs MCTS vs MCTS & 8.0&12.0 &80.0 \\
	\hline
\end{tabularx} 
\end{center}
%TC:endignore

To see how much we improved the RHEA agent, we ran the vanilla version of RHEA with the population size of 1 individual length of 12 and mutation rate of 0.5 as we can see it score a much lower win rate against all the other agents. The performance against MCTS dramatically decreases from \%22 win rate to \%4 win rate while vision range was 2. We will discuss the results in more detail in the following chapters.


\chapter{Discussion }
The observation from our experiments gave many different perspectives about opponent modelling is improving our agent and much room for improvement is left.

\section{Team Mode}
We let the team of two RHEA players compete against four teams of different players individually and also mixed. 
\subsection{Opponent: OSLA} 
The player's winning percentage is improving as we increased the visibility range. When the visibility range was 2, our team of RHEA won 40.0\% of the games and lost 50.0\%. When the range was increased to 4, we saw further increase in performance. Our team won 48.0\%, lost 48.0\% and the remaining was tie. With full vision range, the win percentage was improved by 2.0\% to 50.0\%.  

\subsection{Opponent: Rule Based}
Against Rule-Based players, we have a winning percentage of 48.0\% and a loss percentage of 42.0\% when the visibility range was 2. The winning percentage is further improved to 52.0\%, the tie of 6.0\%  but loss being same. But we saw a small dip in the win with full visibility. The winning percentage was 46.0\%, the tie was 10.0\% and the rest was a loss.

\subsection{Opponent: MCTS}
MCTS is our most competitive opponent. However, we were able to advance our play with the rise in Visibility Range. With a range of 2, we were able to manage to get 30.0\% win, 36.0\% loss and 34.0\% tie. When the range was raised to 4, we saw a 6.0\% increase in win percentage, 36.0\%, the loss remain the same 36.0\% but the tie percentage decreased to 28.0\%. With the further increment of vision range to full, our agent won 34.0\% of games, lost 46.0\% and tied just 20.0\%. With more improvement as described later, we were able to make improvement in our winning percentage.
\subsection{Opponent: MCTS and OSLA}
We were able to win consistently over 60.0\% against this team of MCTS and OSLA. At vision range 2, the win was 64.0\% and the loss was 26.0\%. At vision range 4, the win was 60.0\% and loss was 30.0\%. With the vision range being full, the win was the same 60.0\% but tie at 6.0\% and loss at 34.0\%.

\section{FFA Mode}
Our RHEA player was playing against all the algorithms individually. The results are described as follows

\subsection{Opponent: OSLA}
Our agent was able to dominate the OSLA player in FFA mode. We achieved a total win of 88.0\% at the beginning when the visual range was 2. The performance kept increasing with our agent reaching 94.0\% victory and 6.0\% loss in visual range 4 and we still went on to arrive at 96.0\% and just 4.0\% loss at full visual range. Our agent never tied against OSLA in all three visual ranges against OSLA.

\subsection{Opponent: Rule Based}
We started with a great performance. At visual range being 2, we won 68.0\%, tied 2.0\% and lost in rest. But the performance soared to 84.0\% at visual range 4, tied 4.0\% and lost in 12.0\%. When the visual range was infinite, the win percentage dropped by 2 to 82.0\% but tied in 6.0\% games thus managing the same loss.

\section{Comparisons}
In the FFA mode, we are comparing our agent's performance with the vanilla version of RHEA as well as another agent. Our agent shows a good jump in performance. There is \textbf{33.33\% increase} in win on an average of all the visibility range \textbf{against OSLA}. The increase was \textbf{36\% against Rule-Based and 16\% against MCTS}

\textbf{The performance against MCTS  was greatly improved because of changes in the population size and individual length}. This improved version of our RHEA algorithms implements the Opponent model.

In Team Mode game play, our agent improved with increased visibility. 

\chapter{Conclusions and Future Work} 

\section{Conclusion}
From the above discussions and comparison, we could conclude that we were able to improve the performance of the RHEA agent for the Pommerman with Opponent Modelling aided by weighted randomness and fine-tuning the parameters like the population size, the individual length and different genetic operators. 
We were able to increase the winning percentage of our RHEA agent by 16\% compared to the vanilla version when played against MCTS.
\section{Future Work}
\begin{itemize}
	\item \textbf{Naive Bayes:} For opponent modelling using Naive Bayes, we collected the data of how the other agents are playing the game. We are trying to develop a formula for predicting the action of the opponent using Naive Bayes. We wish to understand the game more and implement a form of Naive Bayes which could take more parameters in the count for Opponent Modelling.
	\item \textbf{Collected Data:} The data that we collected for all the players has a comparatively fewer number of parameters. We want to collect many other parameters that could be included in Naive Bayes.
	\item \textbf{Complexity: }We would like to decrease the average overtime of our agent by decreasing our time complexity and optimising it.
\end{itemize}
	%TC:ignore
	\bibliography{reference}
	\bibliographystyle{ieeetr}
	%TC:endignore
\end{document}
